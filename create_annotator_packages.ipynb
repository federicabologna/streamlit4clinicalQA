{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.getcwd()\n",
    "data_dir = os.path.join(dir, 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "output_dir = os.path.join(dir, 'output')\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create complete annotator files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Annotator1',\n",
       " 'Annotator6',\n",
       " 'Annotator2',\n",
       " 'Annotator3',\n",
       " 'Annotator4',\n",
       " 'Annotator5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(17)\n",
    "\n",
    "annotators = [f\"Annotator{i+1}\" for i in range(6)]\n",
    "random.shuffle(annotators)\n",
    "annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_dict_0 = {'coarse': [0, 50],\n",
    "                 'fine': [50, 100]}\n",
    "\n",
    "indexes_dict_1 = {'coarse': [50, 100],\n",
    "                 'fine': [0, 50]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, \":orange[This is the first sentence.] Here's the second one! Is this the third sentence? Yes, it is.\")\n",
      "(1, \"This is the first sentence. :orange[Here's the second one!] Is this the third sentence? Yes, it is.\")\n",
      "(2, \"This is the first sentence. Here's the second one! :orange[Is this the third sentence?] Yes, it is.\")\n",
      "(3, \"This is the first sentence. Here's the second one! Is this the third sentence? :orange[Yes, it is.]\")\n"
     ]
    }
   ],
   "source": [
    "def bold_sentences(text):\n",
    "    \n",
    "    # Load the spaCy English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Process the text with spaCy to segment into sentences\n",
    "    doc = nlp(text)\n",
    "    sentences = [sentence.text for sentence in doc.sents]\n",
    "    fine_sentences = []\n",
    "    for bold_index in range(len(sentences)):\n",
    "        bold_sentence = f':orange[{sentences[bold_index]}]'\n",
    "        new_sentences = sentences[:bold_index] + [bold_sentence] + sentences[bold_index + 1:]\n",
    "        fine_sentence = ' '.join(new_sentences)\n",
    "        fine_sentences.append((bold_index, fine_sentence))\n",
    "    \n",
    "    return fine_sentences\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"This is the first sentence. Here's the second one! Is this the third sentence? Yes, it is.\"\"\"\n",
    "\n",
    "# bold sentences in the example text\n",
    "for i in bold_sentences(text):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, f\"gpt4_answers.jsonl\"), 'r', encoding='utf-8') as jsonl_file:\n",
    "    gpt4_answers = [json.loads(line) for line in jsonl_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, f\"llama_answers.jsonl\"), 'r', encoding='utf-8') as jsonl_file:\n",
    "    llama_answers = [json.loads(line) for line in jsonl_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, f\"physician_answers.jsonl\"), 'r', encoding='utf-8') as jsonl_file:\n",
    "    physician_answers = [json.loads(line) for line in jsonl_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator1\n",
      "{'coarse': [0, 50], 'fine': [50, 100]}\n",
      "Adding coarse annotations, indexes: [0, 50]\n",
      "Adding fine annotations, indexes: [50, 100]\n",
      "Annotator6\n",
      "{'coarse': [0, 50], 'fine': [50, 100]}\n",
      "Adding coarse annotations, indexes: [0, 50]\n",
      "Adding fine annotations, indexes: [50, 100]\n",
      "Annotator2\n",
      "{'coarse': [0, 50], 'fine': [50, 100]}\n",
      "Adding coarse annotations, indexes: [0, 50]\n",
      "Adding fine annotations, indexes: [50, 100]\n",
      "Annotator3\n",
      "{'coarse': [50, 100], 'fine': [0, 50]}\n",
      "Adding coarse annotations, indexes: [50, 100]\n",
      "Adding fine annotations, indexes: [0, 50]\n",
      "Annotator4\n",
      "{'coarse': [50, 100], 'fine': [0, 50]}\n",
      "Adding coarse annotations, indexes: [50, 100]\n",
      "Adding fine annotations, indexes: [0, 50]\n",
      "Annotator5\n",
      "{'coarse': [50, 100], 'fine': [0, 50]}\n",
      "Adding coarse annotations, indexes: [50, 100]\n",
      "Adding fine annotations, indexes: [0, 50]\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "for annotator in annotators:\n",
    "    \n",
    "    print(annotator)\n",
    "    \n",
    "    if flag < 3:\n",
    "        indexes_dictionary = indexes_dict_0\n",
    "    else:\n",
    "        indexes_dictionary = indexes_dict_1\n",
    "    \n",
    "    print(indexes_dictionary)\n",
    "        \n",
    "    for annotation_type, indexes in indexes_dictionary.items():\n",
    "        \n",
    "        print(f'Adding {annotation_type} annotations, indexes: {indexes}')\n",
    "        \n",
    "        batch_ids = set()\n",
    "        batch = []\n",
    "        \n",
    "        while len(batch_ids) < 3:\n",
    "        \n",
    "            for n in range(indexes[0], indexes[1]):\n",
    "                \n",
    "                qa_pairs = [gpt4_answers[n], llama_answers[n], physician_answers[n]]\n",
    "                random.shuffle(qa_pairs)\n",
    "                \n",
    "                if qa_pairs[0]['question_id'] == qa_pairs[1]['question_id'] == qa_pairs[2]['question_id']:\n",
    "                \n",
    "                    for qa_pair in qa_pairs:\n",
    "                        \n",
    "                        if annotation_type == 'coarse':\n",
    "                            qa_pair['annotation_type'] = annotation_type\n",
    "                            qa_pair['rated'] = 'No'\n",
    "                        \n",
    "                            with open(os.path.join(output_dir, 'all', f'{annotator.lower()}_{annotation_type}.jsonl'), 'a') as file:\n",
    "                                    json.dump(qa_pair, file)\n",
    "                                    file.write('\\n')\n",
    "                    \n",
    "                        elif annotation_type == 'fine':\n",
    "                            sentences = bold_sentences(qa_pair['answer'])\n",
    "                            for sentence in sentences:\n",
    "                                new_d = qa_pair.copy()\n",
    "                                new_d['annotation_type'] = annotation_type\n",
    "                                new_d['rated'] = 'No'\n",
    "                                new_d['sentence_id'] = new_d['answer_id'] + f'_{sentence[0]}'\n",
    "                                new_d['answer'] = sentence[1]\n",
    "                                with open(os.path.join(output_dir, 'all', f'{annotator.lower()}_{annotation_type}.jsonl'), 'a') as file:\n",
    "                                    json.dump(new_d, file)\n",
    "                                    file.write('\\n')\n",
    "\n",
    "    flag += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 'question_49',\n",
       " 'question': 'Does low thyroid contribute to weight gain?',\n",
       " 'answer_id': 'gpt4_49',\n",
       " 'answer': \"Yes, low thyroid function or hypothyroidism can contribute to weight gain. Hypothyroidism slows down the body's metabolism, resulting in a decreased rate at which the body burns calories. This can lead to an increase in body weight. Symptoms of hypothyroidism in addition to weight gain can include fatigue, hair loss, cold intolerance, and constipation, among others. If you suspect you have hypothyroidism, you should visit a healthcare provider for a blood test to measure your thyroid hormone levels. :orange[Treatment typically involves thyroid hormone replacement therapy.]\",\n",
       " 'answer_type': 'gpt4',\n",
       " 'annotation_type': 'fine',\n",
       " 'rated': 'No',\n",
       " 'sentence_id': 'gpt4_49_5'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split annotator files in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ids_per_batch = 3\n",
    "\n",
    "for annotation_type in ['coarse']:#,'fine']:\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'pilot', f\"pilot_{annotation_type}.jsonl\"), 'r', encoding='utf-8') as jsonl_file:\n",
    "        data = [json.loads(line) for line in jsonl_file]\n",
    "    \n",
    "    if annotation_type == 'coarse':\n",
    "        data = data[:9]\n",
    "    else:\n",
    "        data = data[:51]\n",
    "\n",
    "    batch_ids = set()\n",
    "    id_n = 0\n",
    "    batch_id = f'batch_{id_n}'\n",
    "\n",
    "    for d in data:\n",
    "        \n",
    "        if d['question_id'] not in batch_ids and len(batch_ids) == question_ids_per_batch:\n",
    "            batch_ids = set()\n",
    "            id_n += 1\n",
    "            batch_id = f'batch_{id_n}'\n",
    "            \n",
    "        d['batch_id'] = batch_id\n",
    "    \n",
    "    for d in data:\n",
    "        with open(os.path.join(output_dir, 'pilot', f'batches_pilot_{annotation_type}.jsonl'), 'a') as file:\n",
    "            json.dump(d, file)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ids_per_batch = 3\n",
    "\n",
    "for annotation_type in ['coarse']:#,'fine']:\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'all', f\"annotator1_{annotation_type}.jsonl\"), 'r', encoding='utf-8') as jsonl_file:\n",
    "        data = [json.loads(line) for line in jsonl_file]\n",
    "\n",
    "    batch_ids = set()\n",
    "    id_n = 0\n",
    "    batch_id = f'batch_{id_n}'\n",
    "\n",
    "    for d in data:\n",
    "        \n",
    "        if d['question_id'] not in batch_ids and len(batch_ids) == question_ids_per_batch:\n",
    "            batch_ids = set()\n",
    "            id_n += 1\n",
    "            batch_id = f'batch_{id_n}'\n",
    "            \n",
    "        d['batch_id'] = batch_id\n",
    "        batch_ids.add(d['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_0 9\n",
      "batch_1 9\n",
      "batch_2 9\n",
      "batch_3 9\n",
      "batch_4 9\n",
      "batch_5 9\n",
      "batch_6 9\n",
      "batch_7 9\n",
      "batch_8 9\n",
      "batch_9 9\n",
      "batch_10 9\n",
      "batch_11 9\n",
      "batch_12 9\n",
      "batch_13 9\n",
      "batch_14 9\n",
      "batch_15 9\n",
      "batch_16 6\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counting = Counter([d['batch_id'] for d in data])\n",
    "for key, item in counting.items():\n",
    "    print(key, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
